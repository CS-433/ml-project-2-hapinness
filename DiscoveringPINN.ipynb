{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4940a31e",
   "metadata": {},
   "source": [
    "### References\n",
    "##### Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0021999118307125\n",
    "\n",
    "##### Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations\n",
    "\n",
    "https://arxiv.org/pdf/1711.10561.pdf\n",
    "\n",
    "##### Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations\n",
    "\n",
    "https://arxiv.org/pdf/1711.10566.pdf\n",
    "\n",
    "##### Authors: Maziar Raissi, Paris Perdikaris, George Em Karniadakis\n",
    "\n",
    "https://github.com/maziarraissi/PINNs <br>\n",
    "https://maziarraissi.github.io/PINNs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2230a",
   "metadata": {},
   "source": [
    "### 1)Burgers' equation\n",
    "Physics form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + u(t,x) \\frac{\\partial u(t,x)}{\\partial x} = \\nu \\frac{\\partial^2 u(t,x)}{\\partial x^2}$$\n",
    "$u(t,x)$: velocity of fluid, $\\nu$: viscosity of fluid <br><br>\n",
    "\n",
    "General form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + \\lambda _{1} u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\lambda _{2} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$<br>\n",
    "\n",
    "In Maziar Raissi, Paris Perdikaris, and George Em Karniadakis paper: <br>\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\frac{0.01}{\\pi} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$ <br>\n",
    "$$u(0,x) = -sin(\\pi x) \\textrm{, which is the initial condition}$$ <br>\n",
    "$$u(t,-1) = u(t,1) = 0 \\textrm{, which is the Dirichlet boundary conditions}$$<br>\n",
    "$$x \\in [-1,1], \\textrm{ }t\\in [0,1]$$\n",
    "Close to the exact solution is $u(t,x) = e^{-t}sin(\\pi x)$, [here is the exact analytical solution.](https://www.sciencedirect.com/science/article/abs/pii/0045793086900368)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b218c",
   "metadata": {},
   "source": [
    "### 2)PINN approach: Inference => find u(t,x) (Appendix A1 Data-driven solution of partial differential equations)\n",
    "In this part, we are looking to find the exact solution of the PDE (partial derivate equation), $u(t,x)$. The PDE, boundary and inital conditions are known, $u(t,x)$ is unknown.\n",
    "\n",
    "Define $f := u_t + uu_x - \\frac{0.01}{\\pi}u_{xx}$, this function is an approximation of the Burgers' equation / the true PDE given by the PINN. <br>\n",
    "\n",
    "```python\n",
    "def f(t,x):\n",
    "    u=u(t,x)\n",
    "    u_t=tf.gradients(u,t)[0]\n",
    "    u_x=tf.gradients(u,x)[0]\n",
    "    u_xx=tf.gradients(u_x,x)[0]\n",
    "    f=u_t+u∗u_x−(0.01/tf.pi)∗u_xx\n",
    "return f\n",
    "```\n",
    "\n",
    "```u(t,x)``` is the approximation given by the PINN of the solution that satisfies the true PDE. ```u(t,x)``` PINN approximation of $u(t,x)$ the exact solution.\n",
    "\n",
    "```python\n",
    "def u(t,x):\n",
    "    u=neural_net(tf.concat([t,x],1),weights,biases)\n",
    "return u\n",
    "```\n",
    "\n",
    "In other words: ```u(t,x)= PINN(t,x) == PINN u``` and ```f(t,x) == PINN PDE``` <br><br>\n",
    "\n",
    "The loss is defined by: $MSE = MSE_f + MSE_u$, it is a combination of the PDE loss $MSE_f$ and the boundary & initial conditions loss $MSE_u$. <br>\n",
    "$MSE_f = \\frac{1}{N_f}\\Sigma_{i=1}^{N_f} |f(t_f^i,x_f^i)|^2$, where $\\{t_f^i,x_f^i\\}_{i=1}^{N_f}$ is the grid generated for the PDE. <br>\n",
    "$MSE_u = \\frac{1}{N_u}\\Sigma_{i=1}^{N_u} |u(t_u^i,x_u^i) - u^i|^2$, where $\\{t_u^i,x_u^i\\}_{i=1}^{N_u}$ is the grid generated for the boundary & initial conditions, ```u^i``` is an exact solution point from the boundary and initial conditions, ```u(t_u^i,x_u^i)``` is an approximation of $u(t,x)$ (the exact solution) given by the PINN.<br><br>\n",
    "\n",
    "Architecture used:\n",
    "- 9 layers\n",
    "- 20 neurons\n",
    "- LBFGS optimizer\n",
    "- Weights xavier init\n",
    "- Activation function: tanh\n",
    "- Nu = 100: bc and ic points\n",
    "- Nf = 10000: PDE / collocation points\n",
    "\n",
    "Remark: we want to solve the true PDE without knowing its solution, it can be seen as an unsupervised task where the data required is only the boundary and initial conditions (t = 0, x = -1, 1)).\n",
    "\n",
    "**POURQUOI NF EST PAS SUR LE PLOT, en interne dans le NN?**\n",
    "\n",
    "<div>\n",
    "<img src=\"1.PNG\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"2.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af2c8a",
   "metadata": {},
   "source": [
    "### 3)PINN Approach: Identification => find the PDE (Appendix B1 Data-driven discovery of partial differential equations)\n",
    "\n",
    "In this part, the solution $u(x,t)$ is known and we want to discover the parameters of the equation $F(t, x, u(t, x), \\lambda) = 0.$ This is a supervised task, the dataset is constituted of $N$ points, which are randomly selected in the entire space-time domain  $\\Omega = [0, 1] \\times [-1, 1]$, $S = \\{t_i, x_i, u(t_i, x_i)\\}_{i=1}^N$, this dataset gives the solution for every $(t, x) \\in \\Omega$. The information from the boundary & intial conditions is encoded in the dataset, $x \\in [-1,1]$, $t \\in [0,1]$ and $u(t,x)$ in the interior of $\\Omega$. <br>\n",
    "\n",
    "$u(t,x)$ the exact solution is known, the true PDE is unknown, the boundary and inital conditions are known and are only used for the generation of points in $\\Omega$. <br>\n",
    "\n",
    "The general Burgers' equation in 1D is: $$\\frac{\\partial u(t,x)}{\\partial t} + \\lambda _{1} u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\lambda _{2} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$ \n",
    "\n",
    "Define $f := u_t + \\lambda _1 u u_x - \\lambda _2 u_{xx}$, (link with $F$: $\\lambda = (\\lambda_1,\\lambda_2)$), this will be an approximation of the PDE given by the PINN, ```u(t,x)``` approximation of the exact solution given by the PINN.\n",
    "\n",
    "The loss is defined by: $MSE = MSE_f + MSE_u$, it is a combination of the PDE loss $MSE_f$ and the training data loss $MSE_u$. <br>\n",
    "$MSE_f = \\frac{1}{N}\\Sigma_{i=1}^{N} |f(t_u^i,x_u^i)|^2$ <br>\n",
    "$MSE_u = \\frac{1}{N}\\Sigma_{i=1}^{N} |u(t_u^i,x_u^i) - u^i|^2$ <br>\n",
    "\n",
    "Where $\\{t_u^i,x_u^i,u^i\\}_{i=1}^{N}$ is the training dataset.\n",
    "\n",
    "Architecture used:\n",
    "- 9 layers\n",
    "- 20 neurons\n",
    "- LBFGS optimizer\n",
    "- Weights xavier init\n",
    "- Activation function: tanh\n",
    "- N = 2000 \n",
    "\n",
    "Expected $\\lambda$: $\\lambda_1 = 1.0$ and $\\lambda_2 = \\frac{0.01}{\\pi} \\approx 0.00318$\n",
    "\n",
    "<div>\n",
    "<img src=\"3.PNG\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"4.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2128c",
   "metadata": {},
   "source": [
    "### 4)Implementation of Burgers' equation Inference in Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d383f72",
   "metadata": {},
   "source": [
    "intial condition: $u(0,x) = -sin(\\pi x)$ <br>\n",
    "boundary condition 1: $u(t,-1) = 0$ <br>\n",
    "boundary condition 2: $u(t,1) = 0$ <br>\n",
    "$x \\in [-1,1]$ and $t \\in [0,1]$ <br>\n",
    "$N_u = 100$ randomly initial and boundary condtions, which are considered as the training data. <br>\n",
    "$N_f = 10000$ random collocations points, collocations points are points in the domain to test that the solution satisfies the PDE at the collocation points. The collocation points are generated using **latin hypercube sampling** to equally distribute the points in the domain.<br>\n",
    "*Note*: for the collocations points we can add the initial and boundary conditions points. <u>Burgers_shock.mat</u> has $100$ points for $t \\in [0,0.99]$, $256$ points for $x \\in [-1,1]$ and the exact $u(t,x)$ at these points ($25600$ pairs of $(t,x)$). <br><br>\n",
    "Training process:\n",
    "- Create the network, input is a pair $(t,x)$, size 2.\n",
    "- 9 hidden layers with 20 nodes, and tanh as activation function\n",
    "- the output is $u(t,x)$ a real number, size 1.\n",
    "- feedforward the ic/bc and collocations through the PINN\n",
    "- calculate the combined MSE loss = MSEu + MSEf\n",
    "- max_iters is defined in the lbfgs optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6a44ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc5): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc6): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc7): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc8): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc9): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([20, 2])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "#from torch.nn import functional as F #no need if use torch.tanh\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #def __init__(self, n_layers, n_neurons): #add number of layers and neurones as parameters\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() \n",
    "\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 20)\n",
    "        self.fc4 = nn.Linear(20, 20)\n",
    "        self.fc5 = nn.Linear(20, 20)\n",
    "        self.fc6 = nn.Linear(20, 20)\n",
    "        self.fc7 = nn.Linear(20, 20)\n",
    "        self.fc8 = nn.Linear(20, 20)\n",
    "        self.fc9 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        x = torch.tanh(self.fc6(x))\n",
    "        x = torch.tanh(self.fc7(x))\n",
    "        x = torch.tanh(self.fc8(x))\n",
    "        x = torch.tanh(self.fc9(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "#print(len(params))\n",
    "for par in params:\n",
    "    print(par.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5f4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, X_u, u, X_f, lb, ub, nu): #X_u_train, u_train, X_f_train, lb, ub, nu\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"device used: {self.device}\")\n",
    "        \n",
    "        self.lb = torch.tensor(lb).float().to(self.device) #2, [-1,0] xmin, tmin\n",
    "        self.ub = torch.tensor(ub).float().to(self.device) #2 [1,0] xmamx, tmax\n",
    "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the x\n",
    "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the t\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the x\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the t\n",
    "        self.u = torch.tensor(u).float().to(self.device) #100, 1; exact solution for the 100 random ic/bc pairs\n",
    "        self.nu = nu #float 0.01/pi\n",
    "        \n",
    "        self.net = Net().to(self.device)\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        #self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)\n",
    "        \n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.net.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, #1e-8\n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"    \n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t): #get u(x,t) for a pair (x,t); a forward pass through the PINN\n",
    "        u = self.net(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t): #get f = u_t + u * u_x - self.nu * u_xx; the true u(x,t) makes f = 0\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad( #first partial derivative with respect to t\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad( #first partial derivative with respect to x\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad( #second partial derivative with respect to x\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f = u_t + u * u_x - self.nu * u_xx #the computed PDE, we want to be a close as possible to 0\n",
    "        return f\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        u_pred = self.net_u(self.x_u, self.t_u) #100, 1; all the ic/bc pairs were feeded through the PINN\n",
    "        f_pred = self.net_f(self.x_f, self.t_f) #10456, 1; all the collocations points were feeded through the PINN\n",
    "        loss_u = torch.mean((self.u - u_pred) ** 2) #MSE loss on the ic/bc pairs, MSE loss on trainset, classic NN\n",
    "        loss_f = torch.mean(f_pred ** 2) #MSE loss on the collocations pairs, regularization term\n",
    "        \n",
    "        loss = loss_u + loss_f #classic loss + regularization loss (enforce the PDE structure) => PINN loss\n",
    "        \n",
    "        loss.backward() #backpropagation\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(self.device) #get x from pair\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(self.device) #get t from pair\n",
    "\n",
    "        self.net.eval()\n",
    "        u = self.net_u(x, t) \n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff23eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "\n",
    "nu = 0.01/np.pi\n",
    "noise = 0.0        \n",
    "\n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None] #100,1\n",
    "x = data['x'].flatten()[:,None] #256,1\n",
    "Exact = np.real(data['usol']).T #256,100\n",
    "\n",
    "X, T = np.meshgrid(x,t) #X 100,256; T 100,256\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) #25600, 2, exact solution input pairs (t,x)\n",
    "u_star = Exact.flatten()[:,None] #25600, 1, exact solution u(t,x)             \n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(0) #(-1, 0) (xmin, tmin); min(0) for min per column\n",
    "ub = X_star.max(0) #(1, 0.99) (xmax, tmax)\n",
    "\n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) #256, 2; corresponds to the initial condition points u(t=0,x) = -sin(pix)\n",
    "# X[0:1,:].T vector of all the values of x from -1 to 1 of shape (256,1)\n",
    "# T[0:1,:].T vector of 0s of shape (256,1)\n",
    "uu1 = Exact[0:1,:].T #256, 1, exact solution for the initial condition\n",
    "xx2 = np.hstack((X[:,0:1], T[:,0:1])) #100, 2; correspond to the first boundary condition u(t,x=-1) = 0\n",
    "# X[:,0:1] : vector of -1s of shape (100,1)\n",
    "# T[:,0:1] : vector of t values from 0 to 0.99 of shape (100,1)\n",
    "uu2 = Exact[:,0:1] #100, 1; exact solution for the first boundary condition\n",
    "xx3 = np.hstack((X[:,-1:], T[:,-1:])) #100, 2; corresponds to the second boundary condition u(t,x=1) = 0\n",
    "# X[:,-1:] : vector of 1s of shape (100,1)\n",
    "# T[:,-1:] : vector of t values from 0 to 0.99 of shape (100,1)\n",
    "uu3 = Exact[:,-1:] #100, 1; exact solution for the second boundary condition\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3]) #456, 2 #points of initial and boundaries conditions points\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f) #10000, 2 #Latin hypercube sampling: creates 10000 near random pairs of (x,t) in \n",
    "# the domain [-1,1]x[0,0.99], used for PDE\n",
    "X_f_train = np.vstack((X_f_train, X_u_train)) #10456, 2 #stack initial and boundaries points with PDE points, so all the\n",
    "# the initial and boundaries points can also be used for the PDE \n",
    "u_train = np.vstack([uu1, uu2, uu3]) # 456, 1; stack the exact solution points for initial and boundaries conditions\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False) #(100,) 100 permutations from the 456 initial \n",
    "# and boundaries conditions, N_u = 100\n",
    "X_u_train = X_u_train[idx, :] #100,2 randomly points from the 456 initial and boundaries conditions\n",
    "u_train = u_train[idx,:] #100,1 randomly points from the 456 exact solutions points of initial and boundaries conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7755d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#X_u_train 100,2 randomly points from the 456 initial and boundaries conditions, why not used 456,2\n",
    "#u_train 100,1 randomly points from the 456 exact solutions points of initial and boundaries conditions, why not 456,1\n",
    "#X_f_train 10456, 2 #10000 PDE points + 456 initial and boundary conditions points all used for PDE training\n",
    "#lb (-1,0) xmin and tmin\n",
    "#ub (1,0.99) xmax and tmax\n",
    "#nu a float representing 0.01/pi\n",
    "model = PINN(X_u_train, u_train, X_f_train, lb, ub, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190d7725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss: 9.62214e-02, Loss_u: 6.56206e-02, Loss_f: 3.06008e-02\n",
      "Iter 200, Loss: 6.35731e-02, Loss_u: 4.11391e-02, Loss_f: 2.24339e-02\n",
      "Iter 300, Loss: 3.98329e-02, Loss_u: 2.13552e-02, Loss_f: 1.84778e-02\n",
      "Iter 400, Loss: 3.09563e-02, Loss_u: 1.67284e-02, Loss_f: 1.42279e-02\n",
      "Iter 500, Loss: 2.37884e-02, Loss_u: 1.35579e-02, Loss_f: 1.02305e-02\n",
      "Iter 600, Loss: 1.85426e-02, Loss_u: 1.02005e-02, Loss_f: 8.34212e-03\n",
      "Iter 700, Loss: 1.54619e-02, Loss_u: 9.00592e-03, Loss_f: 6.45596e-03\n",
      "Iter 800, Loss: 1.26624e-02, Loss_u: 6.27523e-03, Loss_f: 6.38714e-03\n",
      "Iter 900, Loss: 1.09762e-02, Loss_u: 5.91013e-03, Loss_f: 5.06604e-03\n",
      "Iter 1000, Loss: 9.11567e-03, Loss_u: 4.97978e-03, Loss_f: 4.13589e-03\n",
      "Iter 1100, Loss: 7.35458e-03, Loss_u: 3.69938e-03, Loss_f: 3.65520e-03\n",
      "Iter 1200, Loss: 6.43794e-03, Loss_u: 3.20490e-03, Loss_f: 3.23305e-03\n",
      "Iter 1300, Loss: 5.30934e-03, Loss_u: 2.26982e-03, Loss_f: 3.03952e-03\n",
      "Iter 1400, Loss: 4.36111e-03, Loss_u: 1.91426e-03, Loss_f: 2.44685e-03\n",
      "Iter 1500, Loss: 3.48067e-03, Loss_u: 1.23762e-03, Loss_f: 2.24305e-03\n",
      "Iter 1600, Loss: 2.85627e-03, Loss_u: 1.18463e-03, Loss_f: 1.67164e-03\n",
      "Iter 1700, Loss: 2.48744e-03, Loss_u: 1.00772e-03, Loss_f: 1.47972e-03\n",
      "Iter 1800, Loss: 2.11213e-03, Loss_u: 7.49220e-04, Loss_f: 1.36291e-03\n",
      "Iter 1900, Loss: 1.60817e-03, Loss_u: 5.95436e-04, Loss_f: 1.01273e-03\n",
      "Iter 2000, Loss: 1.43752e-03, Loss_u: 5.14152e-04, Loss_f: 9.23370e-04\n",
      "Iter 2100, Loss: 1.16058e-03, Loss_u: 4.44087e-04, Loss_f: 7.16493e-04\n",
      "Iter 2200, Loss: 1.01501e-03, Loss_u: 4.15837e-04, Loss_f: 5.99178e-04\n",
      "Iter 2300, Loss: 9.15696e-04, Loss_u: 3.79258e-04, Loss_f: 5.36438e-04\n",
      "Iter 2400, Loss: 8.47654e-04, Loss_u: 3.66484e-04, Loss_f: 4.81170e-04\n",
      "Iter 2500, Loss: 7.76840e-04, Loss_u: 3.54457e-04, Loss_f: 4.22383e-04\n",
      "Iter 2600, Loss: 7.19725e-04, Loss_u: 3.31051e-04, Loss_f: 3.88674e-04\n",
      "Iter 2700, Loss: 6.68050e-04, Loss_u: 3.24943e-04, Loss_f: 3.43107e-04\n",
      "Iter 2800, Loss: 6.26214e-04, Loss_u: 3.09529e-04, Loss_f: 3.16685e-04\n",
      "Iter 2900, Loss: 5.90289e-04, Loss_u: 2.96360e-04, Loss_f: 2.93929e-04\n",
      "Iter 3000, Loss: 5.59444e-04, Loss_u: 2.90585e-04, Loss_f: 2.68858e-04\n",
      "Iter 3100, Loss: 5.43026e-04, Loss_u: 2.88895e-04, Loss_f: 2.54130e-04\n",
      "Iter 3200, Loss: 5.18516e-04, Loss_u: 2.73987e-04, Loss_f: 2.44528e-04\n",
      "Iter 3300, Loss: 5.00615e-04, Loss_u: 2.66754e-04, Loss_f: 2.33862e-04\n",
      "Iter 3400, Loss: 4.82440e-04, Loss_u: 2.57867e-04, Loss_f: 2.24572e-04\n",
      "Iter 3500, Loss: 4.66226e-04, Loss_u: 2.53963e-04, Loss_f: 2.12263e-04\n",
      "Iter 3600, Loss: 4.50760e-04, Loss_u: 2.48835e-04, Loss_f: 2.01925e-04\n",
      "Iter 3700, Loss: 4.36859e-04, Loss_u: 2.42716e-04, Loss_f: 1.94143e-04\n",
      "Iter 3800, Loss: 4.22112e-04, Loss_u: 2.40072e-04, Loss_f: 1.82040e-04\n",
      "Iter 3900, Loss: 4.10186e-04, Loss_u: 2.34449e-04, Loss_f: 1.75737e-04\n",
      "Iter 4000, Loss: 3.96159e-04, Loss_u: 2.30853e-04, Loss_f: 1.65306e-04\n",
      "Iter 4100, Loss: 3.84035e-04, Loss_u: 2.29008e-04, Loss_f: 1.55027e-04\n",
      "Iter 4200, Loss: 3.71510e-04, Loss_u: 2.22098e-04, Loss_f: 1.49412e-04\n",
      "Iter 4300, Loss: 3.56385e-04, Loss_u: 2.02779e-04, Loss_f: 1.53606e-04\n",
      "Iter 4400, Loss: 3.37077e-04, Loss_u: 1.93093e-04, Loss_f: 1.43984e-04\n",
      "Iter 4500, Loss: 3.23869e-04, Loss_u: 1.90142e-04, Loss_f: 1.33726e-04\n",
      "Iter 4600, Loss: 3.15195e-04, Loss_u: 1.85144e-04, Loss_f: 1.30050e-04\n",
      "Iter 4700, Loss: 3.03117e-04, Loss_u: 1.81682e-04, Loss_f: 1.21435e-04\n",
      "Iter 4800, Loss: 2.93699e-04, Loss_u: 1.72722e-04, Loss_f: 1.20977e-04\n",
      "Iter 4900, Loss: 2.86579e-04, Loss_u: 1.67646e-04, Loss_f: 1.18933e-04\n",
      "Iter 5000, Loss: 2.79959e-04, Loss_u: 1.64392e-04, Loss_f: 1.15567e-04\n",
      "Iter 5100, Loss: 2.70528e-04, Loss_u: 1.58186e-04, Loss_f: 1.12342e-04\n",
      "Iter 5200, Loss: 2.59211e-04, Loss_u: 1.47284e-04, Loss_f: 1.11927e-04\n",
      "Iter 5300, Loss: 2.51709e-04, Loss_u: 1.44240e-04, Loss_f: 1.07468e-04\n",
      "Iter 5400, Loss: 2.44950e-04, Loss_u: 1.41295e-04, Loss_f: 1.03655e-04\n",
      "Iter 5500, Loss: 2.37510e-04, Loss_u: 1.35450e-04, Loss_f: 1.02060e-04\n",
      "Iter 5600, Loss: 2.30030e-04, Loss_u: 1.31278e-04, Loss_f: 9.87523e-05\n",
      "Iter 5700, Loss: 2.25029e-04, Loss_u: 1.28020e-04, Loss_f: 9.70089e-05\n",
      "Iter 5800, Loss: 2.18421e-04, Loss_u: 1.23910e-04, Loss_f: 9.45112e-05\n",
      "Iter 5900, Loss: 2.12955e-04, Loss_u: 1.23471e-04, Loss_f: 8.94833e-05\n",
      "Iter 6000, Loss: 2.06900e-04, Loss_u: 1.19387e-04, Loss_f: 8.75131e-05\n",
      "Iter 6100, Loss: 2.01226e-04, Loss_u: 1.15798e-04, Loss_f: 8.54279e-05\n",
      "Iter 6200, Loss: 1.97052e-04, Loss_u: 1.16489e-04, Loss_f: 8.05631e-05\n",
      "Iter 6300, Loss: 1.93305e-04, Loss_u: 1.11297e-04, Loss_f: 8.20078e-05\n",
      "Iter 6400, Loss: 1.89244e-04, Loss_u: 1.09634e-04, Loss_f: 7.96102e-05\n",
      "Iter 6500, Loss: 1.85568e-04, Loss_u: 1.06365e-04, Loss_f: 7.92031e-05\n",
      "Iter 6600, Loss: 1.79979e-04, Loss_u: 1.02275e-04, Loss_f: 7.77039e-05\n",
      "Iter 6700, Loss: 1.75670e-04, Loss_u: 9.84783e-05, Loss_f: 7.71914e-05\n",
      "Iter 6800, Loss: 1.70977e-04, Loss_u: 9.62225e-05, Loss_f: 7.47550e-05\n",
      "Iter 6900, Loss: 1.65673e-04, Loss_u: 9.16046e-05, Loss_f: 7.40680e-05\n",
      "Iter 7000, Loss: 1.59727e-04, Loss_u: 8.80451e-05, Loss_f: 7.16817e-05\n",
      "Iter 7100, Loss: 1.53357e-04, Loss_u: 8.39515e-05, Loss_f: 6.94052e-05\n",
      "Iter 7200, Loss: 1.47088e-04, Loss_u: 7.99875e-05, Loss_f: 6.71009e-05\n",
      "Iter 7300, Loss: 1.42381e-04, Loss_u: 7.94785e-05, Loss_f: 6.29029e-05\n",
      "Iter 7400, Loss: 1.37582e-04, Loss_u: 7.43468e-05, Loss_f: 6.32350e-05\n",
      "Iter 7500, Loss: 1.31962e-04, Loss_u: 7.07550e-05, Loss_f: 6.12070e-05\n",
      "Iter 7600, Loss: 1.26165e-04, Loss_u: 6.68194e-05, Loss_f: 5.93459e-05\n",
      "Iter 7700, Loss: 1.21125e-04, Loss_u: 6.07347e-05, Loss_f: 6.03906e-05\n",
      "Iter 7800, Loss: 1.16064e-04, Loss_u: 5.87839e-05, Loss_f: 5.72802e-05\n",
      "Iter 7900, Loss: 1.12070e-04, Loss_u: 5.65190e-05, Loss_f: 5.55506e-05\n",
      "Iter 8000, Loss: 1.06420e-04, Loss_u: 5.25642e-05, Loss_f: 5.38554e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28308/1676823408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28308/744438949.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    424\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0m\u001b[0;32m    427\u001b[0m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0;32m    428\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[1;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mf_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[1;34m(x, t, d)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[1;34m(self, closure, x, t, d)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28308/744438949.py\u001b[0m in \u001b[0;36mloss_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mu_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_u\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#100, 1; all the ic/bc pairs were feeded through the PINN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mf_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_f\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#10456, 1; all the collocations points were feeded through the PINN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mloss_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#MSE loss on the ic/bc pairs, MSE loss on trainset, classic NN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mloss_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_pred\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#MSE loss on the collocations pairs, regularization term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28308/744438949.py\u001b[0m in \u001b[0;36mnet_f\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnet_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#get f = u_t + u * u_x - self.nu * u_xx; the true u(x,t) makes f = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         u_t = torch.autograd.grad( #first partial derivative with respect to t\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30396776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "u_pred, f_pred = model.predict(X_star) #X_star is 25600, 2 contains the pairs who have a exact sol in burgers_shock.mat\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)  #u_star 25600,1 contains the exact solution\n",
    "print('Error u: %e' % (error_u)) #use this metric to compare model performance                   \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(Exact - U_pred) #100, 256 matrix of errors compared to burgers_shock.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0917e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$', size=20)\n",
    "ax.set_ylabel('$x$', size=20)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61999d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 15)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 15)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a66c8",
   "metadata": {},
   "source": [
    "### 5)Implementation of Burgers' equation Inference in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485374e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56adc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033cefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        \n",
    "        # settings\n",
    "        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('lambda_1', self.lambda_1)\n",
    "        self.dnn.register_parameter('lambda_2', self.lambda_2)\n",
    "        \n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = torch.exp(self.lambda_2)\n",
    "        u = self.net_u(x, t)\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        u_pred = self.net_u(self.x, self.t)\n",
    "        f_pred = self.net_f(self.x, self.t)\n",
    "        loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Loss: %e, l1: %.5f, l2: %.5f' % \n",
    "                (\n",
    "                    loss.item(), \n",
    "                    self.lambda_1.item(), \n",
    "                    torch.exp(self.lambda_2.detach()).item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter):\n",
    "            u_pred = self.net_u(self.x, self.t)\n",
    "            f_pred = self.net_f(self.x, self.t)\n",
    "            loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(), \n",
    "                        self.lambda_1.item(), \n",
    "                        torch.exp(self.lambda_2).item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bab37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "\n",
    "N_u = 2000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f896f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "noise = 0.0            \n",
    "\n",
    "# create training set\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# training\n",
    "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "model.train(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "lambda_1_value = model.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value = model.lambda_2.detach().cpu().numpy()\n",
    "lambda_2_value = np.exp(lambda_2_value)\n",
    "\n",
    "error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
    "error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.01    \n",
    "\n",
    "# create training set\n",
    "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "\n",
    "# training\n",
    "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "model.train(10000)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$', size=20)\n",
    "ax.set_ylabel('$x$', size=20)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: u(t,x) slices ################## \n",
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 15)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 15)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluations\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "lambda_1_value_noisy = model.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value_noisy = model.lambda_2.detach().cpu().numpy()\n",
    "lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
    "\n",
    "error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0) * 100\n",
    "error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1_noisy))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2_noisy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### Row 3: Identified PDE ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "gs2 = gridspec.GridSpec(1, 3)\n",
    "gs2.update(top=0.25, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
    "\n",
    "ax = plt.subplot(gs2[:, :])\n",
    "ax.axis('off')\n",
    "\n",
    "s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
    "s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
    "s3 = r'Identified PDE (1\\% noise) & '\n",
    "s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
    "s5 = r'\\end{tabular}$'\n",
    "s = s1+s2+s3+s4+s5\n",
    "ax.text(0.1, 0.1, s, size=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
