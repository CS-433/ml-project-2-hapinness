{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56647bd1",
   "metadata": {},
   "source": [
    "### Burgers' equation\n",
    "Physics form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + u(t,x) \\frac{\\partial u(t,x)}{\\partial x} = \\nu \\frac{\\partial^2 u(t,x)}{\\partial x^2}$$\n",
    "$u(t,x)$: velocity of fluid, $\\nu$: viscosity of fluid <br><br>\n",
    "\n",
    "General form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + \\lambda _{1} u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\lambda _{2} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$<br>\n",
    "\n",
    "In Maziar Raissi, Paris Perdikaris, and George Em Karniadakis paper: <br>\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\frac{0.01}{\\pi} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$ <br>\n",
    "$$u(0,x) = -sin(\\pi x) \\textrm{, which is the initial condition}$$ <br>\n",
    "$$u(t,-1) = u(t,1) = 0 \\textrm{, which is the Dirichlet boundary conditions}$$<br>\n",
    "$$x \\in [-1,1], \\textrm{ }t\\in [0,1]$$\n",
    "Close to the exact solution is $u(t,x) = e^{-t}sin(\\pi x)$, [here is the exact analytical solution.](https://www.sciencedirect.com/science/article/abs/pii/0045793086900368)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b18d65",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d5ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from collections import OrderedDict\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(len(layers) - 2): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (len(layers) - 2), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309021a",
   "metadata": {},
   "source": [
    "### PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8911f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, layers, lambda_1, lambda_2, xt_train, xt_f, u_train, tol=1e-5, hs=50, verbose=True): \n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(f\"device used: {self.device}\")\n",
    "            print(f\"xt_train shape: {xt_train.shape}\")\n",
    "            print(f\"xt_f shape: {xt_f.shape}\")\n",
    "            print(f\"u_train shape: {u_train.shape}\")\n",
    "        \n",
    "        self.net = DNN(layers).to(self.device)\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "        self.x_tr = torch.tensor(xt_train[:, 0:1], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the x\n",
    "        self.t_tr = torch.tensor(xt_train[:, 1:2], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the t\n",
    "        self.x_f = torch.tensor(xt_f[:, 0:1], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the x\n",
    "        self.t_f = torch.tensor(xt_f[:, 1:2], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the t\n",
    "        self.u_tr = torch.tensor(u_train).float().to(self.device) #100, 1; exact solution for the 100 random ic/bc pairs\n",
    "        self.lambda_1 = lambda_1 \n",
    "        self.lambda_2 = lambda_2 \n",
    "        \n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.net.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=hs,\n",
    "            tolerance_grad=tol, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"    \n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t): #get u(x,t) for a pair (x,t); a forward pass through the PINN\n",
    "        u = self.net(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t): #get f = u_t + u * u_x - self.nu * u_xx; the true u(x,t) makes f = 0\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad( #first partial derivative with respect to t\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad( #first partial derivative with respect to x\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad( #second partial derivative with respect to x\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f = u_t + self.lambda_1 * u * u_x - self.lambda_2 * u_xx #the computed PDE, we want to be a close as possible to 0\n",
    "        return f\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        u_pred = self.net_u(self.x_tr, self.t_tr) #100, 1; all the ic/bc pairs were feeded through the PINN\n",
    "        f_pred = self.net_f(self.x_f, self.t_f) #10456, 1; all the collocations points were feeded through the PINN\n",
    "        \n",
    "        loss_u = torch.mean((self.u_tr - u_pred) ** 2) #MSE loss on the ic/bc pairs, MSE loss on trainset, classic NN\n",
    "        loss_f = torch.mean(f_pred ** 2) #MSE loss on the collocations pairs, regularization term\n",
    "        \n",
    "        loss = loss_u + loss_f #classic loss + regularization loss (enforce the PDE structure) => PINN loss\n",
    "        \n",
    "        loss.backward() #backpropagation\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.verbose:\n",
    "            if self.iter % 100 == 0:\n",
    "                print(\n",
    "                    'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "                )\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "            \n",
    "    def predict(self, xt):\n",
    "        x = torch.tensor(xt[:, 0:1], requires_grad=True).float().to(self.device) #get x from pair\n",
    "        t = torch.tensor(xt[:, 1:2], requires_grad=True).float().to(self.device) #get t from pair\n",
    "\n",
    "        self.net.eval()\n",
    "        \n",
    "        u = self.net_u(x, t) \n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3caa44",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "\n",
    "Nu = 100\n",
    "Nf = 10000\n",
    "lambda_1 = 1\n",
    "lambda_2 = 0.01/np.pi\n",
    "\n",
    "sol_data = scipy.io.loadmat('burgers_shock.mat')\n",
    "t_sol = sol_data['t'].flatten()[:,None] #100,1\n",
    "x_sol = sol_data['x'].flatten()[:,None] #256,1\n",
    "X, T = np.meshgrid(x_sol,t_sol) #X 100,256; T 100,256\n",
    "xt_sol = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) #25600, 2, exact solution input pairs (t,x)\n",
    "lower_bound_domain = xt_sol.min(0) #(xmin, tmin)\n",
    "upper_bound_domain = xt_sol.max(0)\n",
    "U_sol = np.real(sol_data['usol']).T #256,100\n",
    "u_sol = U_sol.flatten()[:,None] #25600, 1, exact solution u(t,x)             \n",
    "\n",
    "xt_ic = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "u_ic = -np.sin(xt_ic[:,0:1]*np.pi)\n",
    "xt_bc1 = np.hstack((X[:,0:1], T[:,0:1])) #100, 2; correspond to the first boundary condition u(t,x=-1) = 0\n",
    "u_bc1 = np.zeros(xt_bc1[:,0:1].shape)\n",
    "xt_bc2 = np.hstack((X[:,-1:], T[:,-1:])) #100, 2; corresponds to the second boundary condition u(t,x=1) = 0\n",
    "u_bc2 = np.zeros(xt_bc2[:,0:1].shape)\n",
    "\n",
    "xt_train = np.vstack([xt_ic, xt_bc1, xt_bc2])\n",
    "xt_train_reserve = xt_train\n",
    "u_train = np.vstack([u_ic, u_bc1, u_bc2])\n",
    "u_train_reserve = u_train\n",
    "\n",
    "def gen_train_data(Nu, Nf, lb, ub, xt_tr, u):\n",
    "    xt_f = lb + (ub - lb) * lhs(2, Nf)\n",
    "    xt_f = np.vstack((xt_f, xt_tr)) \n",
    "    idx = np.random.choice(xt_tr.shape[0], Nu, replace=False)\n",
    "    xt_train = xt_tr[idx, :]\n",
    "    u_train = u[idx, :]\n",
    "    return xt_train, xt_f, u_train\n",
    "\n",
    "#data generation without burgers_shock.mat\n",
    "def gen_train_data_2(Nu, Nf, lb, ub): \n",
    "    x_rand = np.random.uniform(low=lb[0], high=ub[0], size=(Nu*2,1))\n",
    "    t_rand = np.random.uniform(low=lb[1], high=ub[1], size=(Nu,1))\n",
    "    xt_ic = np.vstack([x_rand[:,0], np.zeros(x_rand[:,0].shape)]).T\n",
    "    u_ic = (-np.sin(x_rand*np.pi)).reshape(-1,1)\n",
    "    xt_bc1 = np.vstack([np.ones(t_rand[:,0].shape) * -1, t_rand[:,0]]).T\n",
    "    u_bc1 = np.zeros(t_rand.shape)\n",
    "    xt_bc2 = np.vstack([np.ones(t_rand[:,0].shape), t_rand[:,0]]).T\n",
    "    u_bc2 = np.zeros(t_rand.shape)\n",
    "    u_tr = np.vstack([u_ic, u_bc1, u_bc2])\n",
    "    \n",
    "    xt_tr = np.vstack([xt_ic, xt_bc1, xt_bc2])\n",
    "    u_tr = np.vstack([u_ic, u_bc1, u_bc2])\n",
    "    \n",
    "    xt_f = lb + (ub - lb) * lhs(2, Nf)\n",
    "    xt_f = np.vstack((xt_f, xt_tr)) \n",
    "    \n",
    "    idx = np.random.choice(xt_tr.shape[0], Nu, replace=False)\n",
    "    xt_tr = xt_tr[idx, :]\n",
    "    u_tr = u_tr[idx, :]\n",
    "    return xt_tr, xt_f, u_tr\n",
    "\n",
    "xt_train, xt_f, u_train = gen_train_data(Nu, Nf, lower_bound_domain, upper_bound_domain, xt_train, u_train)\n",
    "#xt_train, xt_f, u_train = gen_train_data2(Nu, Nf, lower_bound_domain, upper_bound_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555385d",
   "metadata": {},
   "source": [
    "### Create and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac65d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used: cuda:0\n",
      "xt_train shape: (100, 2)\n",
      "xt_f shape: (10456, 2)\n",
      "u_train shape: (100, 1)\n",
      "Iter 100, Loss: 5.06950e-02, Loss_u: 3.52806e-02, Loss_f: 1.54144e-02\n",
      "Iter 200, Loss: 2.29413e-02, Loss_u: 1.44510e-02, Loss_f: 8.49026e-03\n",
      "Iter 300, Loss: 9.64338e-03, Loss_u: 4.57531e-03, Loss_f: 5.06807e-03\n",
      "Iter 400, Loss: 4.57216e-03, Loss_u: 1.78334e-03, Loss_f: 2.78882e-03\n",
      "Iter 500, Loss: 2.32538e-03, Loss_u: 7.37325e-04, Loss_f: 1.58806e-03\n",
      "Iter 600, Loss: 1.32363e-03, Loss_u: 3.36449e-04, Loss_f: 9.87178e-04\n",
      "Iter 700, Loss: 8.37453e-04, Loss_u: 1.90611e-04, Loss_f: 6.46842e-04\n",
      "Iter 800, Loss: 6.16405e-04, Loss_u: 2.18677e-04, Loss_f: 3.97728e-04\n",
      "Iter 900, Loss: 4.34145e-04, Loss_u: 1.27909e-04, Loss_f: 3.06235e-04\n",
      "Iter 1000, Loss: 3.09860e-04, Loss_u: 8.36737e-05, Loss_f: 2.26186e-04\n",
      "Iter 1100, Loss: 2.49218e-04, Loss_u: 6.08424e-05, Loss_f: 1.88376e-04\n",
      "Iter 1200, Loss: 2.03115e-04, Loss_u: 4.92190e-05, Loss_f: 1.53896e-04\n",
      "Iter 1300, Loss: 1.71912e-04, Loss_u: 3.68920e-05, Loss_f: 1.35020e-04\n",
      "Iter 1400, Loss: 1.44414e-04, Loss_u: 3.46505e-05, Loss_f: 1.09764e-04\n",
      "Iter 1500, Loss: 1.24776e-04, Loss_u: 3.44168e-05, Loss_f: 9.03595e-05\n",
      "Iter 1600, Loss: 1.06945e-04, Loss_u: 2.74399e-05, Loss_f: 7.95048e-05\n",
      "Iter 1700, Loss: 9.43810e-05, Loss_u: 2.12796e-05, Loss_f: 7.31013e-05\n",
      "Iter 1800, Loss: 8.48185e-05, Loss_u: 1.79576e-05, Loss_f: 6.68609e-05\n",
      "Iter 1900, Loss: 7.60577e-05, Loss_u: 1.56227e-05, Loss_f: 6.04351e-05\n",
      "Iter 2000, Loss: 6.95414e-05, Loss_u: 1.44249e-05, Loss_f: 5.51165e-05\n",
      "Iter 2100, Loss: 6.41221e-05, Loss_u: 1.29910e-05, Loss_f: 5.11310e-05\n",
      "Iter 2200, Loss: 5.94442e-05, Loss_u: 1.19141e-05, Loss_f: 4.75301e-05\n",
      "Iter 2300, Loss: 5.45701e-05, Loss_u: 1.11260e-05, Loss_f: 4.34440e-05\n",
      "Iter 2400, Loss: 5.10079e-05, Loss_u: 1.00707e-05, Loss_f: 4.09372e-05\n",
      "Iter 2500, Loss: 4.69789e-05, Loss_u: 8.29092e-06, Loss_f: 3.86880e-05\n",
      "Iter 2600, Loss: 4.37956e-05, Loss_u: 8.19056e-06, Loss_f: 3.56050e-05\n",
      "Iter 2700, Loss: 4.05289e-05, Loss_u: 8.04643e-06, Loss_f: 3.24825e-05\n",
      "Iter 2800, Loss: 3.74528e-05, Loss_u: 7.64052e-06, Loss_f: 2.98123e-05\n",
      "Iter 2900, Loss: 3.45567e-05, Loss_u: 7.18169e-06, Loss_f: 2.73750e-05\n"
     ]
    }
   ],
   "source": [
    "nb_nodes = 20\n",
    "nb_hidden_layers = 9\n",
    "layers = np.hstack([np.array([2]),np.full(nb_hidden_layers, nb_nodes),np.array([1])])\n",
    "\n",
    "#model = PINN(layers, lambda_1, lambda_2, xt_train, xt_f, u_train)\n",
    "model = PINN(layers, lambda_1, lambda_2, xt_train, xt_f, u_train, tol=1e-5, hs=50)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17656e1e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab11eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(pred, true):\n",
    "    return np.linalg.norm(true - pred,2)/np.linalg.norm(true, 2)\n",
    "\n",
    "def mse(pred, true):\n",
    "    return ((pred - true)**2).mean(axis=0).item()\n",
    "\n",
    "u_pred, f_pred = model.predict(xt_sol)\n",
    "\n",
    "l2_u = l2_norm(u_pred, u_sol)  #u_sol 25600,1 contains the exact solution\n",
    "print('L2 Error u: %e' % (l2_u)) \n",
    "\n",
    "mse_u = mse(u_pred, u_sol)\n",
    "mse_f = mse(f_pred, np.zeros(f_pred.shape))\n",
    "\n",
    "print(f\"MSE Error u: {mse_u}\") \n",
    "print(f\"MSE Error f: {mse_f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40ddde",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa516729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "#first plot\n",
    "U_pred = griddata(xt_sol, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t_sol.min(), t_sol.max(), x_sol.min(), x_sol.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    xt_train[:,1], \n",
    "    xt_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "line = np.linspace(x_sol.min(), x_sol.max(), 2)[:,None]\n",
    "ax.plot(t_sol[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t_sol[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t_sol[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$', size=20)\n",
    "ax.set_ylabel('$x$', size=20)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#second plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.remove()\n",
    "\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x_sol,U_sol[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x_sol,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 15)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x_sol,U_sol[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x_sol,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x_sol,U_sol[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x_sol,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 15)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bc4a6",
   "metadata": {},
   "source": [
    "### Network architecture comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60227c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurons = [10, 20, 40]\n",
    "nb_hidden_layers = [2, 4, 6, 8, 10]\n",
    "\n",
    "\n",
    "L2_u_1 = list()\n",
    "MSE_u_1 = list()\n",
    "MSE_f_1 = list()\n",
    "\n",
    "for i in range(len(nb_hidden_layers)):\n",
    "    for j in range(len(nb_neurons)):\n",
    "        layers = np.hstack([np.array([2]),np.full(nb_hidden_layers[i], nb_neurons[j]),np.array([1])])\n",
    "        #verbose = False\n",
    "        model = PINN(layers, lambda_1, lambda_2, xt_train, xt_f, u_train, tol = 1e-5, hs = 50, verbose = False)\n",
    "        model.train()\n",
    "        u_pred, f_pred = model.predict(xt_sol)\n",
    "        L2_u_1.append(l2_norm(u_pred, u_sol))\n",
    "        MSE_u_1.append(mse(u_pred, u_sol))\n",
    "        MSE_f_1.append(mse(f_pred, np.zeros(f_pred.shape)))\n",
    "        print(f\"Model (l={nb_hidden_layers[i]}, n={nb_neurons[j]}), MSEu={MSE_u_1[-1]}, MSEf={MSE_f_1[-1]} \")\n",
    "        \n",
    "\n",
    "best_idx = np.argmin(L2_u_1)\n",
    "best_nb_layers, best_nb_neurons = np.unravel_index(best_idx, (len(nb_hidden_layers),len(nb_neurons)))\n",
    "\n",
    "print(f\"Best performance: l={best_nb_layers}, n={best_nb_neurons}, l2_u={L2_u_1[best_idx]}, mse_u={MSE_u_1[best_idx]}, mse_f={MSE_f_1[best_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c98a3",
   "metadata": {},
   "source": [
    "### Data size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20eae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nu = [10, 50 , 100, 200]\n",
    "Nf = [1000, 2000, 5000, 1000]\n",
    "\n",
    "L2_u_2 = list()\n",
    "MSE_u_2 = list()\n",
    "MSE_f_2 = list()\n",
    "neurons_per_layer = 20\n",
    "hidden_layers = 9\n",
    "layers = np.hstack([np.array([2]),np.full(hidden_layers, neurons_per_layer),np.array([1])])\n",
    "\n",
    "for i in range(len(Nu)):\n",
    "    for j in range(len(Nf)):\n",
    "        xt_train_, xt_f_, u_train_ = gen_train_data(Nu[i], Nf[j], lower_bound_domain, upper_bound_domain, xt_train_reserve, u_train_reserve)\n",
    "        #verbose = False\n",
    "        model = PINN(layers, lambda_1, lambda_2, xt_train_, xt_f_, u_train_, tol=1e-5, hs=50, verbose=False)\n",
    "        model.train()\n",
    "        u_pred, f_pred = model.predict(xt_sol)\n",
    "        L2_u_2.append(l2_norm(u_pred, u_sol))\n",
    "        MSE_u_2.append(mse(u_pred, u_sol))\n",
    "        MSE_f_2.append(mse(f_pred, np.zeros(f_pred.shape)))\n",
    "        print(f\"Data (Nu={Nu[i]}, Nf={Nf[j]}), MSEu={MSE_u_2[-1]}, MSEf={MSE_f_2[-1]} \")\n",
    "\n",
    "        \n",
    "best_idx = np.argmin(L2_u_2)\n",
    "best_nu, best_nf = np.unravel_index(best_idx, (len(Nu),len(Nf)))\n",
    "\n",
    "print(f\"Best performance: Nu={best_nu}, Nf={best_nf}, l2_u={L2_u_2[best_idx]}, mse_u={MSE_u_2[best_idx]}, mse_f={MSE_f_2s[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a69ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e0928e-6b3e-4fcb-adc3-ccc5819a82ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11208/1747887139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_methods\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mxlimits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'smt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "xlimits = np.array([[0.0, 1.0], [0.0, 1.0], [0.0, math.pi]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "\n",
    "num = 50\n",
    "x = sampling(num)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "#plt.plot(x[:, 0], x[:, 1], \"o\")\n",
    "#plt.xlabel(\"x\")\n",
    "#plt.ylabel(\"y\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.9.13"
=======
   "version": "3.9.7"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
