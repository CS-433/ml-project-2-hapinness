{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b72dbf7",
   "metadata": {},
   "source": [
    "### Burgers' equation\n",
    "Physics form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + u(t,x) \\frac{\\partial u(t,x)}{\\partial x} = \\nu \\frac{\\partial^2 u(t,x)}{\\partial x^2}$$\n",
    "$u(t,x)$: velocity of fluid, $\\nu$: viscosity of fluid <br><br>\n",
    "\n",
    "General form:\n",
    "$$\\frac{\\partial u(t,x)}{\\partial t} + \\lambda _{1} u(t,x) \\frac{\\partial u(t,x)}{\\partial x} -\\lambda _{2} \\frac{\\partial^2 u(t,x)}{\\partial x^2} = 0$$<br>\n",
    "\n",
    "In identification, the solution $u(x,t)$ is known and we want to discover the parameters of the equation $F(t, x, u(t, x), \\lambda) = 0.$ This is a supervised task, the dataset is constituted of $N$ points, which are randomly selected in the entire space-time domain  $\\Omega = [0, 1] \\times [-1, 1]$, $S = \\{t_i, x_i, u(t_i, x_i)\\}_{i=1}^N$, this dataset gives the solution for every $(t, x) \\in \\Omega$. The information from the boundary & intial conditions is encoded in the dataset, $x \\in [-1,1]$, $t \\in [0,1]$ and $u(t,x)$ in the interior of $\\Omega$. <br>\n",
    "\n",
    "Define $f := u_t + \\lambda _1 u u_x - \\lambda _2 u_{xx}$, (link with $F$: $\\lambda = (\\lambda_1,\\lambda_2)$), this will be an approximation of the PDE given by the PINN, ```u(t,x)``` approximation of the exact solution given by the PINN.\n",
    "\n",
    "The loss is defined by: $MSE = MSE_f + MSE_u$, it is a combination of the PDE loss $MSE_f$ and the training data loss $MSE_u$. <br>\n",
    "$MSE_f = \\frac{1}{N}\\Sigma_{i=1}^{N} |f(t_u^i,x_u^i)|^2$ <br>\n",
    "$MSE_u = \\frac{1}{N}\\Sigma_{i=1}^{N} |u(t_u^i,x_u^i) - u^i|^2$ <br>\n",
    "\n",
    "Where $\\{t_u^i,x_u^i,u^i\\}_{i=1}^{N}$ is the training dataset. ***Note*: here we use the same training set on both losses.**\n",
    "\n",
    "Expected $\\lambda$: $\\lambda_1 = 1.0$ and $\\lambda_2 = \\frac{0.01}{\\pi} \\approx 0.00318$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f0d3a",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5655f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from collections import OrderedDict\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(len(layers) - 2): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (len(layers) - 2), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145d5fd",
   "metadata": {},
   "source": [
    "### PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, layers, lambda_1, lambda_2, xt_train, xt_f, u_train, tol=1e-5, hs=50, verbose=True): \n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(f\"device used: {self.device}\")\n",
    "            print(f\"xt_train shape: {xt_train.shape}\")\n",
    "            print(f\"xt_f shape: {xt_f.shape}\")\n",
    "            print(f\"u_train shape: {u_train.shape}\")\n",
    "        \n",
    "        self.net = DNN(layers).to(self.device)\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "        self.x_tr = torch.tensor(xt_train[:, 0:1], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the x\n",
    "        self.t_tr = torch.tensor(xt_train[:, 1:2], requires_grad=True).float().to(self.device) #100,1; 100 random ic/bc pairs, take the t\n",
    "        self.x_f = torch.tensor(xt_f[:, 0:1], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the x\n",
    "        self.t_f = torch.tensor(xt_f[:, 1:2], requires_grad=True).float().to(self.device) #10456, 1; 10456 pairs for collocation, take the t\n",
    "        self.u_tr = torch.tensor(u_train).float().to(self.device) #100, 1; exact solution for the 100 random ic/bc pairs\n",
    "        self.lambda_1 = lambda_1 \n",
    "        self.lambda_2 = lambda_2 \n",
    "        \n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.net.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=hs,\n",
    "            tolerance_grad=tol, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"    \n",
    "        )\n",
    "\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t): #get u(x,t) for a pair (x,t); a forward pass through the PINN\n",
    "        u = self.net(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t): #get f = u_t + u * u_x - self.nu * u_xx; the true u(x,t) makes f = 0\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad( #first partial derivative with respect to t\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad( #first partial derivative with respect to x\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad( #second partial derivative with respect to x\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        f = u_t + self.lambda_1 * u * u_x - self.lambda_2 * u_xx #the computed PDE, we want to be a close as possible to 0\n",
    "        return f\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        u_pred = self.net_u(self.x_tr, self.t_tr) #100, 1; all the ic/bc pairs were feeded through the PINN\n",
    "        f_pred = self.net_f(self.x_f, self.t_f) #10456, 1; all the collocations points were feeded through the PINN\n",
    "        \n",
    "        loss_u = torch.mean((self.u_tr - u_pred) ** 2) #MSE loss on the ic/bc pairs, MSE loss on trainset, classic NN\n",
    "        loss_f = torch.mean(f_pred ** 2) #MSE loss on the collocations pairs, regularization term\n",
    "        \n",
    "        loss = loss_u + loss_f #classic loss + regularization loss (enforce the PDE structure) => PINN loss\n",
    "        \n",
    "        loss.backward() #backpropagation\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.verbose:\n",
    "            if self.iter % 100 == 0:\n",
    "                print(\n",
    "                    'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "                )\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "            \n",
    "    def predict(self, xt):\n",
    "        x = torch.tensor(xt[:, 0:1], requires_grad=True).float().to(self.device) #get x from pair\n",
    "        t = torch.tensor(xt[:, 1:2], requires_grad=True).float().to(self.device) #get t from pair\n",
    "\n",
    "        self.net.eval()\n",
    "        \n",
    "        u = self.net_u(x, t) \n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b289ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584ec72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
